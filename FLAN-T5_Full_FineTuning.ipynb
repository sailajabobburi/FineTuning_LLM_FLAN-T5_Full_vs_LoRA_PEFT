{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full Fine-Tuning of FLAN-T5\n",
    "\n",
    "### Overview\n",
    "Previously, we explored zero-shot, one-shot, and few-shot prompting techniques in [Zero_Multishot_Inference.ipynb](./Zero_Multishot_Inference.ipynb).  \n",
    "These approaches demonstrated how well FLAN-T5 can generate responses with pretrained knowledge and in-context learning.\n",
    "\n",
    "Now, we move to full fine-tuning, where we train FLAN-T5 on a specific dataset to improve performance for a targeted task.  \n",
    "Fine-tuning allows the model to:\n",
    "- Learn task-specific patterns beyond its pretraining.\n",
    "- Generalize better within the domain.\n",
    "- Perform more consistently without needing complex prompts.\n",
    "\n",
    "This notebook covers:\n",
    "- Preparing the dataset  \n",
    "- Setting up the fine-tuning pipeline  \n",
    "- Training FLAN-T5 from its pretrained state  \n",
    "- Evaluating and saving the fine-tuned model  \n",
    "\n",
    "Let's begin.\n"
   ],
   "id": "a8466f7d8ae8ac59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:06:53.297074Z",
     "start_time": "2025-03-05T04:06:45.749863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers\n",
    "!pip install datasets --quiet\n",
    "!pip install torchdata\n",
    "!pip install torch\n",
    "!pip install streamlit\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install unstructured\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!pip install evaluate==0.4.0\n",
    "!pip install rouge_score==0.1.2\n",
    "!pip install loralib==0.1.1\n",
    "!pip install peft==0.3.0"
   ],
   "id": "5f5b9628-f0e8-4243-a55e-0365f4dcdb98",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T03:17:44.799784Z",
     "start_time": "2025-03-05T03:17:44.797765Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "df4466e3-0b30-4e1b-a79a-9f6170370afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T03:17:48.744369Z",
     "start_time": "2025-03-05T03:17:46.274723Z"
    }
   },
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Fine Tuning",
   "id": "a2c6aed0-085f-4d58-a8e9-1d1b00ae9b91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T03:17:55.468596Z",
     "start_time": "2025-03-05T03:17:55.455266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE=\"mps\"\n",
    "torch_device = torch.device(DEVICE)"
   ],
   "id": "4abde7b9-a0f0-4f74-a0aa-eaf4994d428d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "75512899-12e6-43fd-98f4-9c7d09847510",
   "metadata": {},
   "source": [
    "## Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa593aee-5a96-46de-8160-bb16abe96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_dataset_name = \"knkarthick/dialogsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a51cc8-d8d3-48b5-a9d0-1da6ad3ae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hugging_face_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc43cb5-7ae6-4921-936a-f6962052d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checking Trainable Parameters\n",
    "\n",
    "This function calculates the total and trainable parameters in the model and their percentage, helping compare full fine-tuning with methods(eg:PEFT).\n"
   ],
   "id": "61cfd4af943d1b60"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ade33c4-b302-4c31-9ff6-c86bfa0efac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
    "        result += f\"all model parameters: {all_model_params}\\n\"\n",
    "        result += f\"Percentage of model params: {(trainable_model_params/all_model_params)*100}\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f1c6a0-f76b-462d-860d-bda4b93a0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "Percentage of model params: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Trainable Parameters\n",
    "\n",
    "In full fine-tuning, we train **all parameters** of the model, which is **247M** here, making the task **resource-intensive** compared to PEFT methods.\n",
    "\n"
   ],
   "id": "c2b6c52d4878dd74"
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2c79-e6ce-49ad-856a-588f61635429",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7fefc6a-53a9-4500-b598-a9ca00a3c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - Zero Shot: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Model Generation - Zero Shot: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22088e95-6104-486c-be81-81194e1fded1",
   "metadata": {},
   "source": [
    "## Perform Full Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd6c4-4932-49b4-b4a1-32a10ce8b533",
   "metadata": {},
   "source": [
    "### Preprocess the Dialog-Summary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8012d-3343-4df0-9063-1aebcdd12bc4",
   "metadata": {},
   "source": [
    "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with 'Summarize the following conversation' and the start of the summary with 'Summary as follows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd762da7-873a-452a-ae18-de765b32a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Handling Train, Validation, and Test Sets\n",
    "\n",
    "The dataset is processed using the `datasets` library, which automatically manages splits:\n",
    "\n",
    "- `dataset[\"train\"]`: The training set used for model learning.\n",
    "- `dataset[\"validation\"]`: The validation set for tuning hyperparameters and preventing overfitting.\n",
    "- `dataset[\"test\"]`: The test set for evaluating final model performance.\n",
    "\n",
    "The `map()` function applies tokenization to all splits while `remove_columns()` ensures only relevant tokenized inputs remain.\n"
   ],
   "id": "7e376c0d4e473867"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f007400f-2603-4ed8-bf20-8ec54199576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12460/12460 [00:07<00:00, 1675.51 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1713.19 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 1795.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test\n",
    "# The tokenize_function code is handling all data accross all splits in batches\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb5aa-bf87-45b0-b90d-a01fcf744d7c",
   "metadata": {},
   "source": [
    "## Subsampling the Dataset\n",
    "\n",
    "To reduce computational load and speed up processing, we select a smaller subset of the dataset instead of using the full data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e36a547-96bf-4bfc-8f4b-bbeed5096e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:07<00:00, 1566.54 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 1348.14 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 1512.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ad14994-44d2-4be0-bbd1-4930cf8b0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e16f70-8638-4e8b-971c-16a09ad9dd13",
   "metadata": {},
   "source": [
    "### Fine-Tune the model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297af38-81f9-4a3e-8394-32e2f536b1af",
   "metadata": {},
   "source": [
    "Now utilize the built-in Hugging Face Trainer class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Arguments and Trainer Setup\n",
    "\n",
    "The `TrainingArguments` define key hyperparameters for training:\n",
    "\n",
    "- **`output_dir`** → Directory to save model checkpoints.  \n",
    "- **`learning_rate`** → Step size for model updates (1e-5).  \n",
    "- **`num_train_epochs`** → Number of full passes over the dataset (1 epoch).  \n",
    "- **`weight_decay`** → Regularization to prevent overfitting (0.01).  \n",
    "- **`logging_steps`** → Logs training progress after every step (1).  \n",
    "- **`max_steps`** → Limits training to 1 step for quick testing.\n",
    "\n",
    "The `Trainer` manages training and evaluation:\n",
    "- **`model`** → The FLAN-T5 model.  \n",
    "- **`train_dataset`** → Training dataset.  \n",
    "- **`eval_dataset`** → Validation dataset for evaluation.  \n"
   ],
   "id": "da2342ddc5e71777"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "210e9712-d0a5-43a4-b041-0284f44f057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f8bcb-eea6-48ce-90bc-490ea7175f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc796ce0-a496-428c-85eb-f3b7f18a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir).to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c322fc2-062c-443d-a8b1-eddd3ca7741a",
   "metadata": {},
   "source": [
    "## Evaluate the Model Qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51766726-3189-4b08-82cc-ab9ccab2ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Generation - Zero Shot: \n",
      "#Person1#: You can consider upgrading your system to a more powerful and more powerful hard disk.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Generation - Fine Tune: \n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person1# also suggests #Person2# add a CD-ROM drive to #Person2#'s computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Fine Tune: \\n{instruct_text_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6bb6-0889-4fe2-9771-a5fc50560215",
   "metadata": {},
   "source": [
    "## Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be96e9ae-014c-4e89-b80e-1c97aa78ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db6a8655-c9cd-4c12-ae09-81ad9532bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_text_output)\n",
    "\n",
    "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "821b77dd-2618-4d82-bc00-f6c5fda06ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>original</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to use instant messagin...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo will be sent to all employees by thi...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees are required to use the Office of In...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>People are talking about the traffic in this c...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>#Person1: I'm finally here!</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1: I'm sorry to hear that you're stuck ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: Masha and Hero are getting a divorce.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Brian, thank you for coming to the ...</td>\n",
       "      <td>Brian's birthday is coming. Brian dances with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               human  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Employees are required to use instant messagin...   \n",
       "1  This memo will be sent to all employees by thi...   \n",
       "2  Employees are required to use the Office of In...   \n",
       "3  People are talking about the traffic in this c...   \n",
       "4                        #Person1: I'm finally here!   \n",
       "5  #Person1: I'm sorry to hear that you're stuck ...   \n",
       "6                       Masha and Hero are divorced.   \n",
       "7                       Masha and Hero are divorced.   \n",
       "8    #Person1: Masha and Hero are getting a divorce.   \n",
       "9  #Person1#: Brian, thank you for coming to the ...   \n",
       "\n",
       "                                            instruct  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again. #Person1...  \n",
       "4  #Person2# got stuck in traffic again. #Person1...  \n",
       "5  #Person2# got stuck in traffic again. #Person1...  \n",
       "6  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "7  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "8  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "9  Brian's birthday is coming. Brian dances with ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b55838ad-c8bc-4e05-a94a-ddff480da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d075dc6-4c89-4ea4-b8bb-0b81e5fe9cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.261052062988671, 'rouge2': 0.08531489481944488, 'rougeL': 0.224821552384684, 'rougeLsum': 0.22788611265447228}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.38857220563277894, 'rouge2': 0.13135692283806472, 'rougeL': 0.28167162470172985, 'rougeLsum': 0.28344342480768214}\n"
     ]
    }
   ],
   "source": [
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance Improvement and Training Time\n",
    "\n",
    "The **fine-tuned model** shows a **significant improvement** in ROUGE scores over the zero-shot model, especially in **ROUGE-1 (+12.7%)** and **ROUGE-2 (+4.6%)**, indicating better summarization quality.\n",
    "\n",
    "### Training Time:\n",
    "- **Device:** Mac M2 (`mps`)\n",
    "- **Epochs:** 1\n",
    "- **Total Time:** ~24 hours"
   ],
   "id": "dfbc86960660d648"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion: Full Fine-Tuning of FLAN-T5\n",
    "\n",
    "In this notebook, we successfully fine-tuned **FLAN-T5** for dialogue summarization. \n",
    "\n",
    "### **Key Takeaways:**\n",
    "- **Zero-shot vs. Fine-Tuned Performance:**  \n",
    "  - The fine-tuned model achieved **significant improvements** in ROUGE scores over zero-shot inference.  \n",
    "- **Training Challenges:**  \n",
    "  - Fine-tuning on **Mac M2 (`mps') took ~24 hours** even with dataset downsizing.  \n",
    "- **Optimization Strategies:**  \n",
    "  - Techniques like **LoRA/PEFT, quantization, and hyperparameter tuning** can improve efficiency."
   ],
   "id": "3e4713ab4ea11729"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
