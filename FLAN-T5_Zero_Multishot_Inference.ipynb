{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploring Zero-Shot and Multi-Shot Prompting with FLAN-T5\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we explore different prompting strategies—**zero-shot** and **multi-shot prompting**—to generate dialogue summaries using a **pre-trained Large Language Model (LLM), FLAN-T5** from Hugging Face.\n",
    "\n",
    "### Objective\n",
    "We aim to:\n",
    "- Generate dialogue summaries using **zero-shot** and **multi-shot** prompting.\n",
    "- Compare the effectiveness of both approaches.\n",
    "- Evaluate performance using **ROUGE scores**.\n",
    "\n",
    "### Dataset\n",
    "We will use the **DialogSum** dataset from Hugging Face, which contains over 10,000 dialogues with manually labeled summaries.\n",
    "\n",
    "### Model\n",
    "For this task, we will use **FLAN-T5**, a fine-tuned variant of T5 optimized for instruction-following and reasoning tasks.\n",
    "\n",
    "\n"
   ],
   "id": "67402e773ff9f66f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T04:06:38.203388Z",
     "start_time": "2025-03-05T04:06:30.404481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers\n",
    "!pip install datasets --quiet\n",
    "!pip install torchdata\n",
    "!pip install torch\n",
    "!pip install streamlit\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install unstructured\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!pip install evaluate==0.4.0\n",
    "!pip install rouge_score==0.1.2\n",
    "!pip install loralib==0.1.1\n",
    "!pip install peft==0.3.0"
   ],
   "id": "5f5b9628-f0e8-4243-a55e-0365f4dcdb98",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:06.635472Z",
     "start_time": "2025-03-04T23:42:06.631832Z"
    }
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "df4466e3-0b30-4e1b-a79a-9f6170370afc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:10.381200Z",
     "start_time": "2025-03-04T23:42:08.096298Z"
    }
   },
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "0e802765-6ded-4432-b620-11f9baad4649",
   "metadata": {},
   "source": [
    "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10.000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd9b5056-8e47-40cb-ad13-b96b9783c9e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:13.190960Z",
     "start_time": "2025-03-04T23:42:13.188497Z"
    }
   },
   "source": [
    "DEVICE=\"mps\"\n",
    "torch_device = torch.device(DEVICE)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "38ecc2b7-676c-412f-aeb5-912767751f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:20.410996Z",
     "start_time": "2025-03-04T23:42:15.516805Z"
    }
   },
   "source": [
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.65k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea0342515580486380fc89cfcdd887a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50b3f15af1a549aca86248b8406574cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.3M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2decc8891c9c4255a225f7cd6013a0a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cacde869f17431f8ddbe5003fd56660"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f690db72ef384095b1685185279da2f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1baa473ecd254ce9b5cca0eeaad82f3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2bd33013e5e4806800ca748fa3366dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d347f5ea5cb4533824c87aa16d1a72d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27893b6a0b784bf0b8ac6d6326241abe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "cafeed4a-0ad3-4180-be88-6c6b97085973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:22.899013Z",
     "start_time": "2025-03-04T23:42:22.896941Z"
    }
   },
   "source": [
    "example_indices = [40, 200]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a411d4a8-0bc1-42d9-80c2-8e9b1b4d9a60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:42:24.814879Z",
     "start_time": "2025-03-04T23:42:24.804571Z"
    }
   },
   "source": [
    "dash_line = \"-\"*100\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print('Input Dialogue: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('Baseline Human Summary: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "    "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "4f8ea871-a66d-4ed2-b1e7-ed703930dfd6",
   "metadata": {},
   "source": [
    "## **FLAN-T5 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23f87-1cf0-4e65-bfc4-3606c32fbac6",
   "metadata": {},
   "source": "![FLAN-T5 Architecture](images/flan2_architecture.jpg)"
  },
  {
   "cell_type": "code",
   "id": "0d95e03b-8148-418c-9259-114693afd5ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T23:46:46.705117Z",
     "start_time": "2025-03-04T23:46:23.770500Z"
    }
   },
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b50aad3324944b529f058605283378cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d79a8dae3b424a92970198c3649d4721"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "229b3478e8d749acacbcc835471946cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7662df3c6b074e65be1176cdda1fcd25"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9adc163064e4d15b96149b9c83775df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f22ac2cdb89a4a6780522d17d8c7f824"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0f13a66d8db48ea8c59b085094fdac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "e9248fbf-1a34-4f29-90d0-070b1b126f2d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "91e93706c99bad63"
  },
  {
   "cell_type": "markdown",
   "id": "f0e4ccb4-e1ab-47be-8578-330a8428da07",
   "metadata": {},
   "source": "![FLAN-T5 Tasks](images/flan_t5_tasks.png)"
  },
  {
   "cell_type": "markdown",
   "id": "83dd6c0c-a017-41ec-b760-7d9cfc2b55cf",
   "metadata": {},
   "source": [
    "#### These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35639b0a-188c-4333-917f-2843259a7cea",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24014c68-c37c-4acb-81f3-b08ffb476ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it, Tom?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c0ca1ec-8652-4eeb-9fc6-3edc65dd806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      " tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "DECODED SENTENCE: What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(f'ENCODED SENTENCE:\\n {sentence_encoded[\"input_ids\"][0]}')\n",
    "print(f'DECODED SENTENCE: {sentence_decoded}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero-Shot Prompting for Dialogue Summarization\n",
    "\n",
    "In this section, we use **zero-shot prompting**, where the model generates summaries **without any prior examples or fine-tuning**. We provide the raw dialogue as input and let **FLAN-T5** generate a summary based purely on its pretrained knowledge.\n"
   ],
   "id": "e5ace819f009e874"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc69b96d-4fb1-4ebd-9834-e0385dca2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dialogues(example_indices, dataset, prompt = \"%s\"):\n",
    "    for i, index in enumerate(example_indices):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "    \n",
    "        input = prompt % (dialogue)\n",
    "        \n",
    "        inputs = tokenizer(input, return_tensors='pt')\n",
    "        pred = model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0]\n",
    "        output = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "    \n",
    "        print(dash_line)\n",
    "        print(f'Example {i+1}')\n",
    "        print(dash_line)\n",
    "        print(f'Input Prompt: \\n {dialogue}')\n",
    "        print(dash_line)\n",
    "        print(f'Baseline Human Summary: \\n {summary}')\n",
    "        print(dash_line)\n",
    "        print(f'Model Generation: \\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa3b893f-c33f-4084-a61a-eafd80bec0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b2c41-41c5-4d5c-b1db-11eb3eefb906",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0849bb4-948f-48dd-af41-0d0e52252715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. \n",
      "%s\n",
      "Summary:\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Summarize the following conversation. \\n%s\\nSummary:'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b179486b-f899-4fb7-b04b-c514c5d1e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb3417f3-9ca3-425a-820d-ae1bf425a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: \n",
      "%s\n",
      "\n",
      "What Happened?\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Dialogue: \\n%s\\n\\nWhat Happened?'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e845923b-e26d-4c1e-9d69-a7a9e4e70450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "Tom is late.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: You'd probably need a faster processor, more memory and a faster modem. #Person2#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e38693-945c-4000-bd83-5912db40047a",
   "metadata": {},
   "source": [
    "## One-Shot Inference\n",
    "\n",
    "In **one-shot prompting**, we provide the model with **a single example** (dialogue + summary) before asking it to generate a summary for a new dialogue.\n",
    "\n",
    "This method helps the model better understand the expected output format and improves response quality compared to **zero-shot prompting**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f58f375e-557d-4a14-aefa-ad97f264f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"Dialogue:\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\\n\\n\"\"\"\n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f'Dialogue:\\n{dialogue}\\n\\nWhat was going on?'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c70dcf8-1a76-4dbb-ae4f-d94212f73660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfff3608-270f-4415-ab80-84118ff1845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - One Shot:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - One Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758b8a-5e70-4562-987c-6fba74c087c2",
   "metadata": {},
   "source": [
    "## Few-Shot Inference\n",
    "\n",
    "In **few-shot prompting**, we provide the model with **multiple examples** (dialogue-summary pairs) before asking it to generate a summary for a new dialogue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3790c048-358f-4e04-9306-57d24d759c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a22b0ef-d18a-4915-8b51-1f909bdb6ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - Few Shot:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - Few Shot:\\n{output}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary: Comparing Zero-Shot, One-Shot, and Few-Shot Dialogue Summarization\n",
    "\n",
    "### **Observations:**\n",
    "- **Zero-Shot Output:** The model struggles to fully understand the task without examples, often producing incomplete or generic summaries.\n",
    "- **One-Shot Output:** Adding a single example improves structure, but inconsistencies still appear.\n",
    "- **Few-Shot Output:** Performance improves further, generating more structured summaries, but results may still lack **task-specific accuracy**.\n",
    "\n",
    "### **Performance Improvement & Limitations**\n",
    "- **Few-shot prompting enhances response quality** but still has **limitations:**\n",
    "  - Requires longer context, which may **exceed token limits**.\n",
    "  - Can still produce **inconsistent outputs**.\n",
    "  - May not **fully adapt to domain-specific data**.\n",
    "\n",
    "### **When to Choose Fine-Tuning Over Prompting?**\n",
    "- **Fine-tuning is necessary when:**\n",
    "  - The task requires **domain-specific language** not well-covered in pretraining.\n",
    "  - **Prompting isn't reliable** across multiple cases.\n",
    "  - You need **consistent behavior** and performance across a dataset.\n",
    "  - **Inference cost needs to be optimized**, as fine-tuned models require **fewer tokens per request** compared to multi-shot prompting.\n",
    "\n",
    "### **Next Steps**\n",
    "To fully explore fine-tuning, refer to the following notebooks:\n",
    "- **[Placeholder: Full Fine-Tuning Notebook]**\n",
    "- **[Placeholder: PEFT/LoRA Fine-Tuning Notebook]**\n",
    "\n",
    "These will provide an end-to-end implementation of full fine-tuning and **parameter-efficient fine-tuning (PEFT)** methods like **LoRA** to further enhance model performance.\n"
   ],
   "id": "907353256a928e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c874856bf3608fd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
