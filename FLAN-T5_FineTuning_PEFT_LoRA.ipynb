{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parameter-Efficient Fine-Tuning (PEFT) for FLAN-T5\n",
    "\n",
    "### **Overview**\n",
    "In this notebook, we explore **PEFT (Parameter-Efficient Fine-Tuning)** methods to fine-tune **FLAN-T5** with significantly fewer trainable parameters.\n",
    "\n",
    "### **Why PEFT?**\n",
    "- **Full fine-tuning is computationally expensive** (high memory, long training time).\n",
    "- PEFT methods like **LoRA (Low-Rank Adaptation)** fine-tune only a subset of parameters.\n",
    "- This results in **faster training, lower memory usage, and efficient deployment**.\n",
    "\n",
    "### **What We Will Do:**\n",
    "1. Implement **LoRA** for efficient fine-tuning.\n",
    "2. Compare PEFT performance against:\n",
    "   - **Full Fine-Tuning** ([`FLAN-T5_Full_FineTuning.ipynb`](./FLAN-T5_Full_FineTuning.ipynb))\n",
    "   - **Zero/Multi-Shot Inference** ([`FLAN-T5_Zero_MultiShot_Inference.ipynb`](./FLAN-T5_Zero_MultiShot_Inference.ipynb))\n",
    "3. Evaluate training time, model performance, and resource savings.\n",
    "\n",
    "This approach enables fine-tuning on **low-resource hardware** while maintaining high performance.\n"
   ],
   "id": "eef760fed97c66a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# python- 3.8.10 \n",
    "# !pip install --upgrade pip\n",
    "# !pip install transformers\n",
    "# !pip install datasets --quiet\n",
    "# !pip install torchdata\n",
    "# !pip install torch\n",
    "# !pip install streamlit\n",
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install unstructured\n",
    "# !pip install sentence-transformers\n",
    "# !pip install chromadb\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install rouge_score==0.1.2\n",
    "# !pip install loralib==0.1.1\n",
    "# !pip install peft==0.3.0"
   ],
   "id": "5f5b9628-f0e8-4243-a55e-0365f4dcdb98"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4466e3-0b30-4e1b-a79a-9f6170370afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 33,
   "source": [
    "DEVICE=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_device = torch.device(DEVICE)"
   ],
   "id": "4abde7b9-a0f0-4f74-a0aa-eaf4994d428d"
  },
  {
   "cell_type": "markdown",
   "id": "75512899-12e6-43fd-98f4-9c7d09847510",
   "metadata": {},
   "source": [
    "## Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa593aee-5a96-46de-8160-bb16abe96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_dataset_name = \"knkarthick/dialogsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a51cc8-d8d3-48b5-a9d0-1da6ad3ae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hugging_face_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc43cb5-7ae6-4921-936a-f6962052d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "def number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
    "        result += f\"all model parameters: {all_model_params}\\n\"\n",
    "        result += f\"Percentage of model params: {(trainable_model_params/all_model_params)*100}\"\n",
    "        return result"
   ],
   "id": "7ade33c4-b302-4c31-9ff6-c86bfa0efac5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2c79-e6ce-49ad-856a-588f61635429",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd6c4-4932-49b4-b4a1-32a10ce8b533",
   "metadata": {},
   "source": [
    "### Preprocess the Dialog-Summary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8012d-3343-4df0-9063-1aebcdd12bc4",
   "metadata": {},
   "source": [
    "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with 'Summarize the following conversation' and the start of the summary with 'Summary as follows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd762da7-873a-452a-ae18-de765b32a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f007400f-2603-4ed8-bf20-8ec54199576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12460/12460 [00:07<00:00, 1675.51 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1713.19 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 1795.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test\n",
    "# The tokenize_function code is handling all data accross all splits in batches\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb5aa-bf87-45b0-b90d-a01fcf744d7c",
   "metadata": {},
   "source": [
    "To save some time, we will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e36a547-96bf-4bfc-8f4b-bbeed5096e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:07<00:00, 1566.54 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 1348.14 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 1512.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ad14994-44d2-4be0-bbd1-4930cf8b0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fine-Tune the model with the Preprocessed Dataset",
   "id": "d2e16f70-8638-4e8b-971c-16a09ad9dd13"
  },
  {
   "cell_type": "markdown",
   "id": "7297af38-81f9-4a3e-8394-32e2f536b1af",
   "metadata": {},
   "source": [
    "Now utilize the built-in Hugging Face Trainer class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 44,
   "source": [
    "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ],
   "id": "210e9712-d0a5-43a4-b041-0284f44f057b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()  ",
   "id": "902f8bcb-eea6-48ce-90bc-490ea7175f6d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 46,
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('full/').to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ],
   "id": "dc796ce0-a496-428c-85eb-f3b7f18a4f11"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be96e9ae-014c-4e89-b80e-1c97aa78ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 49,
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_text_output)\n",
    "\n",
    "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'instruct'])"
   ],
   "id": "db6a8655-c9cd-4c12-ae09-81ad9532bc74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ef21e62fc36e734f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>original</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to use instant messagin...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo will be sent to all employees by thi...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees are required to use the Office of In...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>People are talking about the traffic in this c...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>#Person1: I'm finally here!</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1: I'm sorry to hear that you're stuck ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: Masha and Hero are getting a divorce.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Brian, thank you for coming to the ...</td>\n",
       "      <td>Brian's birthday is coming. Brian dances with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               human  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Employees are required to use instant messagin...   \n",
       "1  This memo will be sent to all employees by thi...   \n",
       "2  Employees are required to use the Office of In...   \n",
       "3  People are talking about the traffic in this c...   \n",
       "4                        #Person1: I'm finally here!   \n",
       "5  #Person1: I'm sorry to hear that you're stuck ...   \n",
       "6                       Masha and Hero are divorced.   \n",
       "7                       Masha and Hero are divorced.   \n",
       "8    #Person1: Masha and Hero are getting a divorce.   \n",
       "9  #Person1#: Brian, thank you for coming to the ...   \n",
       "\n",
       "                                            instruct  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again. #Person1...  \n",
       "4  #Person2# got stuck in traffic again. #Person1...  \n",
       "5  #Person2# got stuck in traffic again. #Person1...  \n",
       "6  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "7  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "8  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "9  Brian's birthday is coming. Brian dances with ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50,
   "source": "df",
   "id": "821b77dd-2618-4d82-bc00-f6c5fda06ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 51,
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")"
   ],
   "id": "b55838ad-c8bc-4e05-a94a-ddff480da233"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n"
   ],
   "id": "0d075dc6-4c89-4ea4-b8bb-0b81e5fe9cda",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13167f8b-86c3-4c2f-b7b6-b615b78178c7",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine Tunning with LoRA\n",
    "\n",
    "\n",
    "## **Example: LoRA vs. Full Fine-Tuning**\n",
    "Suppose we have a model layer with a weight matrix **W** of size **(1000 × 1000)**.  \n",
    "- In **full fine-tuning**, we update all **1,000,000 parameters** in **W**.  \n",
    "- With **LoRA (r=16)**, we replace updates to **W** with two much smaller matrices:  \n",
    "  - **A (1000 × 16)**\n",
    "  - **B (16 × 1000)**  \n",
    "- Instead of updating **1,000,000 parameters**, LoRA only trains **32,000 parameters** → **96.8% fewer parameters!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5747ce62-a989-4221-8533-db2d8af27f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup the PEFT/LoRA model for Fine-Tunning\n",
    "## LoRA Configuration Parameters Explained\n",
    "\n",
    "### 1.`r=32` → Rank of the LoRA Matrices  \n",
    "- Controls the size of the low-rank adaptation matrices (`A` and `B`).  \n",
    "- **Higher `r`** → More trainable parameters, better learning but higher memory usage.  \n",
    "- **Lower `r`** → Fewer parameters, faster training but may reduce model adaptability.  \n",
    "- **Impact:** A trade-off between efficiency and model performance.  \n",
    "\n",
    "### 2.`lora_alpha=32` → Scaling Factor  \n",
    "- `lora_alpha` scales the LoRA update (`ΔW = α/r * A * B`).  \n",
    "- Ensures the LoRA matrices don't overly distort the original frozen model.  \n",
    "- **Higher `lora_alpha`** → Increases impact of LoRA updates.  \n",
    "- **Lower `lora_alpha`** → Reduces adaptation strength.  \n",
    "- **Impact:** Adjusts the balance between adaptation flexibility and stability.  \n",
    "\n",
    "### 3.`target_modules=['q', 'v']` → Which Layers to Apply LoRA To  \n",
    "- LoRA is applied only to the **attention mechanism** of the transformer.  \n",
    "- `\"q\"` → Query matrix (controls how words attend to each other).  \n",
    "- `\"v\"` → Value matrix (controls what information gets passed in attention).  \n",
    "- **Impact:**  \n",
    "  - Applying LoRA only to `\"q\"` and `\"v\"` reduces compute cost while maintaining performance.  \n",
    "  - Adding `\"k\"` (Key) and `\"o\"` (Output) would fine-tune more components but use more memory.  \n",
    "\n",
    "### 4. `lora_dropout=0.05` → Dropout for Regularization  \n",
    "- Introduces **random dropping of LoRA updates** to prevent overfitting.  \n",
    "- **Higher dropout (`>0.1`)** → Reduces overfitting but may slow learning.  \n",
    "- **Lower dropout (`<0.05`)** → Retains more updates but risks overfitting.  \n",
    "- **Impact:** Helps prevent the LoRA parameters from overfitting on small datasets.  \n",
    "\n",
    "### 5. bias='none'` → Whether to Fine-Tune Bias Terms  \n",
    "- `\"none\"` → Biases remain frozen (**default, saves memory**).  \n",
    "- `\"all\"` → Fine-tunes all biases (**increases trainable parameters**).  \n",
    "- `\"lora_only\"` → Fine-tunes only biases within LoRA layers.  \n",
    "- **Impact:** Keeping biases frozen reduces computational cost while maintaining efficiency.  \n",
    "\n",
    "### 6. `task_type=TaskType.SEQ_2_SEQ_LM` → Task Type for Fine-Tuning  \n",
    "- Specifies that we are fine-tuning a **sequence-to-sequence language model** (e.g., FLAN-T5).  \n",
    "- Other options include:  \n",
    "  - `\"CAUSAL_LM\"` → For **autoregressive models** like GPT.  \n",
    "  - `\"SEQ_2_SEQ_LM\"` → For **encoder-decoder models** like T5 that generate output from input.  \n",
    "- **Impact:** Ensures LoRA is correctly applied based on the model architecture.  \n"
   ],
   "id": "6ef8166e-6506-4ad8-a1b0-b1256965a65e"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ba24b4e-5bba-465e-90c0-054bea5a24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q','v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1122785-3957-41a9-b1d9-844d7d7da279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "Percentage of model params: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "With **LoRA fine-tuning**, we reduced the trainable parameters from **251M to just 3.5M**, making up only **1.41%** "
   ],
   "id": "a9246598af07a465"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f35033e6-9d34-4874-8f30-da488b5001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da1282-c082-488b-a831-cdffc948e1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "601c74cb-571b-46be-affc-13c4f995309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    output_dir\n",
    ").to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### PEFT model Training Time:\n",
    "- **Device:** Mac M2 (`mps`)\n",
    "- **Epochs:** 1\n",
    "- **Total Time:** ~7 hours"
   ],
   "id": "763003a7f3c825e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Qualitative Evaluation  \n",
    "\n",
    "We assess the model's inference by comparing the model generated summary against the human-written summary, checking for fluency, accuracy, conciseness, and relevance.\n"
   ],
   "id": "1098aa247ff55ce6"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d51e485a-a14b-4150-af1d-681509b8a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Generation - Zero Shot: \n",
      "#Person2#: I'm not sure what exactly I need. #Person1: I'm not sure what I need. #Person2: I'm just not sure. #Person1#: I'm not sure what I need. #Person1: I'm not sure what I need. #Person2: I'm not sure what I need. #Person1: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person1: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person1: I'm just not sure what I need.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Generation - Zero Shot: \n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system and #Person2# thinks it's a definite bonus. #Person1# also suggests adding a painting program to #Person2#'s software. #Person2# also wants to upgrade #Person2#'s hardware.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Zero Shot: \\n{peft_text_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aeaeac7f-aa42-4e44-afd8-83ae481c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'peft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8def102-fa83-4131-868d-6c8313adc587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.261052062988671, 'rouge2': 0.08531489481944488, 'rougeL': 0.224821552384684, 'rougeLsum': 0.22788611265447228}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.38857220563277894, 'rouge2': 0.13135692283806472, 'rougeL': 0.28167162470172985, 'rougeLsum': 0.28344342480768214}\n",
      "Peft Model: \n",
      "{'rouge1': 0.33176278482581173, 'rouge2': 0.08811333505050914, 'rougeL': 0.2509677309788697, 'rougeLsum': 0.25262149176905513}\n"
     ]
    }
   ],
   "source": [
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n",
    "print(f\"Peft Model: \\n{peft_model_results}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparing Models Using ROUGE Scores  \n",
    "\n",
    "We compare the **zero-shot (original), fully fine-tuned (instruct), and PEFT fine-tuned (LoRA) models** using ROUGE scores.  \n",
    "\n",
    "### **ROUGE Score Comparison:**\n",
    "| Model            | ROUGE-1 ↑  | ROUGE-2 ↑  | ROUGE-L ↑  | ROUGE-Lsum ↑ |\n",
    "|-----------------|------------|------------|------------|--------------|\n",
    "| **Original Model**  | **0.261**  | **0.085**  | **0.225**  | **0.228**  |\n",
    "| **Fully Fine-Tuned (Instruct)** | **0.389**  | **0.131**  | **0.282**  | **0.283**  |\n",
    "| **PEFT (LoRA) Model** | **0.332**  | **0.088**  | **0.251**  | **0.253**  |\n",
    "\n",
    "### **Observations:**\n",
    "- **Fully fine-tuned (instruct) model performs best**, with **ROUGE-1 improving by +12.7%** over the original model.  \n",
    "- **PEFT (LoRA) model shows a 7% improvement over the original model** while training far fewer parameters.  \n",
    "- **LoRA fine-tuning achieves competitive performance** compared to full fine-tuning, but with significantly lower computational cost.\n",
    "## Training Limitations and Future Improvements  \n",
    "Due to computational constraints, we fine-tuned the model for **only one epoch**.  \n",
    "The model's performance can be **further improved** by training for **more epochs**, allowing it to learn better representations and refine summaries.\n",
    "\n"
   ],
   "id": "5c405b4e20ac8ee9"
  },
  {
   "cell_type": "markdown",
   "id": "61b809f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
